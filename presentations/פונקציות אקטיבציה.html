<!DOCTYPE html>
<html lang="he" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>פונקציות אקטיבציה</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .presentation-container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .slide {
            display: none;
            padding: 40px;
            min-height: 600px;
        }
        
        .slide.active {
            display: block;
        }
        
        .slide h1 {
            color: #2c3e50;
            font-size: 2.5em;
            margin-bottom: 30px;
            text-align: center;
            border-bottom: 3px solid #3498db;
            padding-bottom: 15px;
        }
        
        .slide h2 {
            color: #34495e;
            font-size: 2em;
            margin-bottom: 20px;
            border-right: 5px solid #e74c3c;
            padding-right: 15px;
        }
        
        .slide h3 {
            color: red;
            font-size: 1.4em;
            margin-bottom: 15px;
        }
        
        .function-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
            margin-top: 30px;
        }
        
        .function-card {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border-radius: 15px;
            padding: 25px;
            box-shadow: 0 8px 15px rgba(0,0,0,0.1);
            transition: transform 0.3s ease;
        }
        
        .function-card:hover {
            transform: translateY(-5px);
        }
        
        .function-name {
            font-size: 1.3em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 15px;
        }
        
        .function-graph {
            width: 100%;
            height: 200px;
            background: white;
            border-radius: 10px;
            margin: 15px 0;
            position: relative;
            overflow: hidden;
        }
        
        .graph-canvas {
            width: 100%;
            height: 100%;
        }
        
        .formula {
            background: #34495e;
            color: white;
            padding: 10px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            text-align: center;
            margin: 10px 0;
        }
        
        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-top: 15px;
        }
        
        .pros, .cons {
            padding: 15px;
            border-radius: 10px;
        }
        
        .pros {
            background: #d5f4e6;
            border-right: 4px solid #27ae60;
        }
        
        .cons {
            background: #fdeaea;
            border-right: 4px solid #e74c3c;
        }
        
        .pros h4, .cons h4 {
            margin: 0 0 10px 0;
            font-size: 1.1em;
        }
        
        .pros h4 {
            color: #27ae60;
        }
        
        .cons h4 {
            color: #e74c3c;
        }
        
        .navigation {
            background: #2c3e50;
            padding: 20px;
            text-align: center;
        }
        
        .nav-button {
            background: #3498db;
            color: white;
            border: none;
            padding: 12px 25px;
            margin: 0 10px;
            border-radius: 25px;
            cursor: pointer;
            font-size: 1em;
            transition: background 0.3s ease;
        }
        
        .nav-button:hover {
            background: #2980b9;
        }
        
        .nav-button:disabled {
            background: #95a5a6;
            cursor: not-allowed;
        }
        
        .slide-counter {
            color: white;
            margin: 0 20px;
            font-size: 1.1em;
        }
        
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }
        
        .comparison-table th, .comparison-table td {
            padding: 15px;
            text-align: center;
            border-bottom: 1px solid #ddd;
        }
        
        .comparison-table th {
            background: #34495e;
            color: white;
            font-weight: bold;
        }
        
        .comparison-table tr:nth-child(even) {
            background: #f8f9fa;
        }
        
        .use-case {
            background: #e8f5e8;
            border-right: 4px solid #2ecc71;
            padding: 20px;
            margin: 15px 0;
            border-radius: 10px;
        }
        
        .use-case h4 {
            color: #27ae60;
            margin: 0 0 10px 0;
        }
        
        .intro-text {
            font-size: 1.2em;
            line-height: 1.6;
            color: #34495e;
            text-align: center;
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin: 30px 0;
        }
    </style>
</head>
<body>
    <div class="presentation-container">
        <!-- Slide 1: Title -->
        <div class="slide active">
            <h1>פונקציות אקטיבציה</h1>
            <div class="intro-text">
                <p>המפתח להפיכת הרשתות העצביות לחכמות</p>
                <p>מדריך מקיף לתלמידים</p>
            </div>
            <div style="text-align: center; margin-top: 50px;">
                <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: yellow; padding: 30px; border-radius: 15px; display: inline-block;">
                    <h3>מה נלמד היום?</h3>
                    <p>✓ מה זה פונקציית אקטיבציה ולמה היא חשובה</p>
                    <p>✓ הכרת הפונקציות השונות וההבדלים ביניהן</p>
                    <p>✓ איך לבחור את הפונקציה הנכונה לכל מצב</p>
                    <p>✓ דוגמאות מעשיות ויישומים</p>
                </div>
            </div>
        </div>

        <!-- Slide 2: Introduction -->
        <div class="slide">
            <h1>מה זה פונקציית אקטיבציה?</h1>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 30px;">
                <div>
                    <h3>המטרה:</h3>
                    <p>פונקציית האקטיבציה מחליטה האם נוירון "מופעל" או לא, על בסיס הקלט שהוא מקבל.</p>
                    
                    <h3>למה זה חשוב?</h3>
                    <ul>
                        <li>מאפשר לרשת ללמוד דפוסים מורכבים</li>
                        <li>הופך בעיות ליניאריות לבעיות לא-ליניאריות</li>
                        <li>שולט בזרימת המידע ברשת</li>
                    </ul>
                </div>
                <div>
                    <div style="background: #f8f9fa; padding: 20px; border-radius: 15px; text-align: center;">
                        <h4>אנלוגיה:</h4>
                        <p>דמיין נוירון כמו מתג חשמלי חכם:</p>
                        <p>🔹 מקבל אותות מכמה מקורות</p>
                        <p>🔹 מחליט האם להעביר אות הלאה</p>
                        <p>🔹 פונקציית האקטיבציה היא הלוגיקה של ההחלטה</p>
                    </div>
                    
                    <div style="background: #e8f5e8; padding: 20px; border-radius: 15px; margin-top: 20px;">
                        <h4>בלי פונקציית אקטיבציה:</h4>
                        <p>הרשת העצבית תהיה רק מערך של משוואות ליניאריות - לא תוכל ללמוד דפוסים מורכבים!</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 3: Sigmoid -->
        <div class="slide">
            <h1>Sigmoid - הקלאסית</h1>
            <div class="function-card">
                <div class="function-name">פונקציית Sigmoid</div>
                <div class="formula">f(x) = 1 / (1 + e^(-x))</div>
                <div class="function-graph">
                    <canvas class="graph-canvas" id="sigmoid-canvas"></canvas>
                </div>
                <div class="pros-cons">
                    <div class="pros">
                        <h4>✓ יתרונות:</h4>
                        <p>• פלט בין 0 ו-1 (מושלם להסתברויות)</p>
                        <p>• חלק וגזיר</p>
                        <p>• פשוט להבנה</p>
                    </div>
                    <div class="cons">
                        <h4>✗ חסרונות:</h4>
                        <p>• Vanishing Gradient Problem</p>
                        <p>• לא ממורכז סביב 0</p>
                        <p>• חישוב יקר (exponential)</p>
                    </div>
                </div>
                <div class="use-case">
                    <h4>מתי נשתמש?</h4>
                    <p>• שכבת הפלט בבעיות binary classification</p>
                    <p>• כאשר צריך פלט שמייצג הסתברות</p>
                    <p>• רשתות רדודות</p>
                </div>
            </div>
        </div>

        <!-- Slide 4: Tanh -->
        <div class="slide">
            <h1>Tanh - השיפור של Sigmoid</h1>
            <div class="function-card">
                <div class="function-name">פונקציית Tanh</div>
                <div class="formula">f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</div>
                <div class="function-graph">
                    <canvas class="graph-canvas" id="tanh-canvas"></canvas>
                </div>
                <div class="pros-cons">
                    <div class="pros">
                        <h4>✓ יתרונות:</h4>
                        <p>• פלט בין -1 ו-1 (ממורכז סביב 0)</p>
                        <p>• גרדיאנט חזק יותר מ-Sigmoid</p>
                        <p>• חלק וגזיר</p>
                    </div>
                    <div class="cons">
                        <h4>✗ חסרונות:</h4>
                        <p>• עדיין סובל מ-Vanishing Gradient</p>
                        <p>• חישוב יקר</p>
                        <p>• לא מתאים לרשתות עמוקות</p>
                    </div>
                </div>
                <div class="use-case">
                    <h4>מתי נשתמש?</h4>
                    <p>• שכבות נסתרות ברשתות רדודות</p>
                    <p>• כאשר רוצים פלט ממורכז סביב 0</p>
                    <p>• RNN ו-LSTM (במקומות מסוימים)</p>
                </div>
            </div>
        </div>

        <!-- Slide 5: ReLU -->
        <div class="slide">
            <h1>ReLU - המהפכה</h1>
            <div class="function-card">
                <div class="function-name">פונקציית ReLU</div>
                <div class="formula">f(x) = max(0, x)</div>
                <div class="function-graph">
                    <canvas class="graph-canvas" id="relu-canvas"></canvas>
                </div>
                <div class="pros-cons">
                    <div class="pros">
                        <h4>✓ יתרונות:</h4>
                        <p>• פשוט וזריז מאוד</p>
                        <p>• פותר את בעיית Vanishing Gradient</p>
                        <p>• מאפשר רשתות עמוקות</p>
                        <p>• יוצר sparsity (חלק מהנוירונים כבויים)</p>
                    </div>
                    <div class="cons">
                        <h4>✗ חסרונות:</h4>
                        <p>• Dying ReLU Problem</p>
                        <p>• לא גזיר ב-0</p>
                        <p>• פלט לא מוגבל</p>
                    </div>
                </div>
                <div class="use-case">
                    <h4>מתי נשתמש?</h4>
                    <p>• הברירת מחדל לרוב הרשתות העמוקות</p>
                    <p>• CNN, Deep Neural Networks</p>
                    <p>• כמעט תמיד בשכבות נסתרות</p>
                </div>
            </div>
        </div>

        <!-- Slide 6: Leaky ReLU -->
        <div class="slide">
            <h1>Leaky ReLU - התיקון הקטן</h1>
            <div class="function-card">
                <div class="function-name">פונקציית Leaky ReLU</div>
                <div class="formula">f(x) = max(αx, x), α = 0.01</div>
                <div class="function-graph">
                    <canvas class="graph-canvas" id="leaky-relu-canvas"></canvas>
                </div>
                <div class="pros-cons">
                    <div class="pros">
                        <h4>✓ יתרונות:</h4>
                        <p>• פותר את בעיית Dying ReLU</p>
                        <p>• כמעט זריז כמו ReLU</p>
                        <p>• מאפשר למידה מערכים שליליים</p>
                    </div>
                    <div class="cons">
                        <h4>✗ חסרונות:</h4>
                        <p>• פרמטר נוסף (α) לכוונון</p>
                        <p>• לא תמיד טוב יותר מ-ReLU</p>
                    </div>
                </div>
                <div class="use-case">
                    <h4>מתי נשתמש?</h4>
                    <p>• כאשר ReLU גורם לנוירונים "למות"</p>
                    <p>• רשתות עם הרבה נתונים שליליים</p>
                    <p>• כחלופה ל-ReLU כשהביצועים לא טובים</p>
                </div>
            </div>
        </div>

        <!-- Slide 7: ELU -->
        <div class="slide">
            <h1>ELU - הגרסה החלקה</h1>
            <div class="function-card">
                <div class="function-name">פונקציית ELU</div>
                <div class="formula">f(x) = x if x > 0, α(e^x - 1) if x ≤ 0</div>
                <div class="function-graph">
                    <canvas class="graph-canvas" id="elu-canvas"></canvas>
                </div>
                <div class="pros-cons">
                    <div class="pros">
                        <h4>✓ יתרונות:</h4>
                        <p>• חלק בכל מקום (גזיר)</p>
                        <p>• פלט ממוצע קרוב ל-0</p>
                        <p>• לא סובל מ-Dying ReLU</p>
                    </div>
                    <div class="cons">
                        <h4>✗ חסרונות:</h4>
                        <p>• חישוב יקר יותר (exponential)</p>
                        <p>• פרמטר נוסף לכוונון</p>
                    </div>
                </div>
                <div class="use-case">
                    <h4>מתי נשתמש?</h4>
                    <p>• כאשר רוצים פונקציה חלקה</p>
                    <p>• רשתות שצריכות התכנסות מהירה</p>
                    <p>• כאשר יש זמן חישוב מספיק</p>
                </div>
            </div>
        </div>

        <!-- Slide 8: Swish -->
        <div class="slide">
            <h1>Swish - החדשנות של Google</h1>
            <div class="function-card">
                <div class="function-name">פונקציית Swish</div>
                <div class="formula">f(x) = x * sigmoid(x) = x / (1 + e^(-x))</div>
                <div class="function-graph">
                    <canvas class="graph-canvas" id="swish-canvas"></canvas>
                </div>
                <div class="pros-cons">
                    <div class="pros">
                        <h4>✓ יתרונות:</h4>
                        <p>• ביצועים טובים ברשתות עמוקות</p>
                        <p>• חלק וגזיר</p>
                        <p>• התנהגות טובה בערכים שליליים</p>
                    </div>
                    <div class="cons">
                        <h4>✗ חסרונות:</h4>
                        <p>• חישוב יקר (sigmoid)</p>
                        <p>• לא תמיד עדיף על ReLU</p>
                        <p>• חדש יחסית</p>
                    </div>
                </div>
                <div class="use-case">
                    <h4>מתי נשתמש?</h4>
                    <p>• רשתות עמוקות מאוד</p>
                    <p>• כאשר ReLU לא מספיק טוב</p>
                    <p>• מודלים מתקדמים (EfficientNet)</p>
                </div>
            </div>
        </div>

        <!-- Slide 9: Comparison Table -->
        <div class="slide">
            <h1>השוואה מקיפה</h1>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>פונקציה</th>
                        <th>מהירות</th>
                        <th>רשתות עמוקות</th>
                        <th>Vanishing Gradient</th>
                        <th>Dying Problem</th>
                        <th>שימוש עיקרי</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Sigmoid</strong></td>
                        <td>🔴 איטי</td>
                        <td>🔴 לא</td>
                        <td>🔴 כן</td>
                        <td>🟢 לא</td>
                        <td>פלט בינארי</td>
                    </tr>
                    <tr>
                        <td><strong>Tanh</strong></td>
                        <td>🔴 איטי</td>
                        <td>🔴 לא</td>
                        <td>🟡 פחות</td>
                        <td>🟢 לא</td>
                        <td>שכבות נסתרות</td>
                    </tr>
                    <tr>
                        <td><strong>ReLU</strong></td>
                        <td>🟢 מהיר</td>
                        <td>🟢 כן</td>
                        <td>🟢 לא</td>
                        <td>🔴 כן</td>
                        <td>ברירת מחדל</td>
                    </tr>
                    <tr>
                        <td><strong>Leaky ReLU</strong></td>
                        <td>🟢 מהיר</td>
                        <td>🟢 כן</td>
                        <td>🟢 לא</td>
                        <td>🟢 לא</td>
                        <td>תיקון ReLU</td>
                    </tr>
                    <tr>
                        <td><strong>ELU</strong></td>
                        <td>🟡 בינוני</td>
                        <td>🟢 כן</td>
                        <td>🟢 לא</td>
                        <td>🟢 לא</td>
                        <td>רשתות חלקות</td>
                    </tr>
                    <tr>
                        <td><strong>Swish</strong></td>
                        <td>🟡 בינוני</td>
                        <td>🟢 כן</td>
                        <td>🟢 לא</td>
                        <td>🟢 לא</td>
                        <td>מודלים מתקדמים</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <!-- Slide 10: Decision Guide -->
        <div class="slide">
            <h1>איך לבחור? מדריך החלטות</h1>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 30px;">
                <div>
                    <h2>🎯 לפי סוג הבעיה</h2>
                    
                    <div class="use-case">
                        <h4>Classification בינארי</h4>
                        <p>שכבת פלט: <strong>Sigmoid</strong></p>
                        <p>שכבות נסתרות: <strong>ReLU</strong></p>
                    </div>
                    
                    <div class="use-case">
                        <h4>Multi-class Classification</h4>
                        <p>שכבת פלט: <strong>Softmax</strong></p>
                        <p>שכבות נסתרות: <strong>ReLU</strong></p>
                    </div>
                    
                    <div class="use-case">
                        <h4>Regression</h4>
                        <p>שכבת פלט: <strong>Linear</strong> (ללא אקטיבציה)</p>
                        <p>שכבות נסתרות: <strong>ReLU</strong></p>
                    </div>
                </div>
                
                <div>
                    <h2>🏗️ לפי ארכיטקטורה</h2>
                    
                    <div class="use-case">
                        <h4>CNN (Convolutional)</h4>
                        <p>בדרך כלל: <strong>ReLU</strong></p>
                        <p>מתקדם: <strong>Leaky ReLU</strong></p>
                    </div>
                    
                    <div class="use-case">
                        <h4>RNN/LSTM</h4>
                        <p>Gates: <strong>Sigmoid</strong></p>
                        <p>פלט: <strong>Tanh</strong></p>
                    </div>
                    
                    <div class="use-case">
                        <h4>Deep Networks (עמוקות)</h4>
                        <p>התחלה: <strong>ReLU</strong></p>
                        <p>בעיות: <strong>Leaky ReLU</strong></p>
                        <p>מתקדם: <strong>Swish</strong></p>
                    </div>
                </div>
            </div>
            
            <div style="background: #f8f9fa; padding: 25px; border-radius: 15px; margin-top: 30px;">
                <h3 style="color: #2c3e50; text-align: center;">💡 כלל הזהב</h3>
                <p style="text-align: center; font-size: 1.2em;">
                    <strong>התחל עם ReLU</strong> - אם לא עובד טוב, נסה <strong>Leaky ReLU</strong>.<br>
                    רק אם באמת צריך - עבור לפונקציות מורכבות יותר.
                </p>
            </div>
        </div>

        <!-- Slide 11: Practical Tips -->
        <div class="slide">
            <h1>טיפים מעשיים</h1>
            
            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 30px;">
                <div>
                    <h2>🔧 שיטות עבודה</h2>
                    
                    <div style="background: #e8f5e8; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                        <h4>1. אל תשנה הכל בבת אחת</h4>
                        <p>נסה פונקציות שונות אחת אחת וראה מה עובד</p>
                    </div>
                    
                    <div style="background: #e8f5e8; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                        <h4>2. שים לב לזמן אימון</h4>
                        <p>Sigmoid ו-Tanh יכולים להיות איטיים מאוד</p>
                    </div>
                    
                    <div style="background: #e8f5e8; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                        <h4>3. עקוב אחרי Loss</h4>
                        <p>אם ה-Loss לא יורד - אולי בעיה באקטיבציה</p>
                    </div>
                </div>
                
                <div>
                    <h2>⚠️ שגיאות נפוצות</h2>
                    
                    <div style="background: #fdeaea; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                        <h4>❌ ReLU בשכבת פלט</h4>
                        <p>לא מתאים לרוב הבעיות - הפלט לא מוגבל</p>
                    </div>
                    
                    <div style="background: #fdeaea; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                        <h4>❌ התעלמות מ-Dying ReLU</h4>
                        <p>אם הרבה נוירונים "מתים" - עבור ל-Leaky ReLU</p>
                    </div>
                </div>
            </div>
            
            <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 25px; border-radius: 15px; margin-top: 30px;">
                <h3 style="text-align: center; margin-bottom: 20px;">🎓 לסיכום</h3>
                <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;">
                    <div style="text-align: center;">
                        <h4>🚀 למתחילים</h4>
                        <p>ReLU בכל מקום<br>(חוץ משכבת פלט)</p>
                    </div>
                    <div style="text-align: center;">
                        <h4>🔬 למתקדמים</h4>
                        <p>נסו Leaky ReLU<br>או ELU</p>
                    </div>
                    <div style="text-align: center;">
                        <h4>🏆 למומחים</h4>
                        <p>Swish למודלים<br>חדשניים</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 12: Summary -->
        <div class="slide">
            <h1>סיכום - מה למדנו?</h1>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 40px; margin-top: 30px;">
                <div>
                    <h2>🧠 מושגי מפתח</h2>
                    <div style="background: #f8f9fa; padding: 20px; border-radius: 10px;">
                        <p><strong>Vanishing Gradient:</strong> כאשר הגרדיאנט הופך חלש מדי ברשתות עמוקות</p>
                        <p><strong>Dying ReLU:</strong> נוירונים שמפסיקים ללמוד כי הם תמיד מחזירים 0</p>
                        <p><strong>ליניאריות:</strong> בלי אקטיבציה, הרשת לא תוכל ללמוד דפוסים מורכבים</p>
                        <p><strong>Sparsity:</strong> חלק מהנוירונים כבויים - יכול להיות טוב או רע</p>
                    </div>
                </div>
                
                <div>
                    <h2>📋 רשימת ביקורת</h2>
                    <div style="background: #e8f5e8; padding: 20px; border-radius: 10px;">
                        <p>✅ בחרתי את הפונקציה הנכונה לבעיה שלי?</p>
                        <p>✅ שכבת הפלט מתאימה לסוג הבעיה?</p>
                        <p>✅ הרשת מתכנסת (Loss יורד)?</p>
                        <p>✅ אין בעיות של Dying neurons?</p>
                        <p>✅ זמן האימון סביר?</p>
                        <p>✅ הביצועים טובים על validation?</p>
                    </div>
                </div>
            </div>
            
            <div style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; padding: 30px; border-radius: 15px; margin-top: 40px; text-align: center;">
                <h2>🎯 המסר העיקרי</h2>
                <p style="font-size: 1.3em; margin: 20px 0;">
                    פונקציית האקטיבציה היא לב הרשת העצבית
                </p>
                <p style="font-size: 1.1em;">
                    הבחירה הנכונה יכולה להיות ההבדל בין הצלחה לכישלון במודל
                </p>
                <div style="margin-top: 30px;">
                    <p><strong>זכרו:</strong> תמיד התחילו פשוט (ReLU) ושפרו בהדרגה!</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Navigation -->
    <div class="navigation">
        <button class="nav-button" onclick="previousSlide()">← קודם</button>
        <span class="slide-counter">
            <span id="current-slide">1</span> / <span id="total-slides">12</span>
        </span>
        <button class="nav-button" onclick="nextSlide()">הבא →</button>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        
        document.getElementById('total-slides').textContent = totalSlides;
        
        function showSlide(n) {
            slides[currentSlide].classList.remove('active');
            currentSlide = (n + totalSlides) % totalSlides;
            slides[currentSlide].classList.add('active');
            document.getElementById('current-slide').textContent = currentSlide + 1;
            
            // Update navigation buttons
            const prevBtn = document.querySelector('.nav-button:first-child');
            const nextBtn = document.querySelector('.nav-button:last-child');
            
            prevBtn.disabled = currentSlide === 0;
            nextBtn.disabled = currentSlide === totalSlides - 1;
            
            // Draw graphs for current slide
            drawGraphs();
        }
        
        function nextSlide() {
            if (currentSlide < totalSlides - 1) {
                showSlide(currentSlide + 1);
            }
        }
        
        function previousSlide() {
            if (currentSlide > 0) {
                showSlide(currentSlide - 1);
            }
        }
        
        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowRight' || e.key === ' ') {
                nextSlide();
            } else if (e.key === 'ArrowLeft') {
                previousSlide();
            }
        });
        
        // Graph drawing functions
        function drawGraphs() {
            const canvases = document.querySelectorAll('.graph-canvas');
            canvases.forEach(canvas => {
                if (canvas.offsetParent !== null) { // Only draw visible canvases
                    const id = canvas.id;
                    switch(id) {
                        case 'sigmoid-canvas':
                            drawSigmoid(canvas);
                            break;
                        case 'tanh-canvas':
                            drawTanh(canvas);
                            break;
                        case 'relu-canvas':
                            drawReLU(canvas);
                            break;
                        case 'leaky-relu-canvas':
                            drawLeakyReLU(canvas);
                            break;
                        case 'elu-canvas':
                            drawELU(canvas);
                            break;
                        case 'swish-canvas':
                            drawSwish(canvas);
                            break;
                    }
                }
            });
        }
        
        function setupCanvas(canvas) {
            const ctx = canvas.getContext('2d');
            canvas.width = canvas.offsetWidth;
            canvas.height = canvas.offsetHeight;
            
            // Clear canvas
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            
            // Draw axes
            const centerX = canvas.width / 2;
            const centerY = canvas.height / 2;
            
            ctx.strokeStyle = '#ddd';
            ctx.lineWidth = 1;
            
            // X-axis
            ctx.beginPath();
            ctx.moveTo(0, centerY);
            ctx.lineTo(canvas.width, centerY);
            ctx.stroke();
            
            // Y-axis
            ctx.beginPath();
            ctx.moveTo(centerX, 0);
            ctx.lineTo(centerX, canvas.height);
            ctx.stroke();
            
            return { ctx, centerX, centerY };
        }
        
        function drawFunction(canvas, func, color = '#e74c3c', lineWidth = 3) {
            const { ctx, centerX, centerY } = setupCanvas(canvas);
            
            ctx.strokeStyle = color;
            ctx.lineWidth = lineWidth;
            ctx.beginPath();
            
            const scale = 50;
            let first = true;
            
            for (let x = -canvas.width/2; x < canvas.width/2; x += 1) {
                const realX = x / scale;
                const realY = func(realX);
                const screenX = centerX + x;
                const screenY = centerY - realY * scale;
                
                if (screenY >= 0 && screenY <= canvas.height) {
                    if (first) {
                        ctx.moveTo(screenX, screenY);
                        first = false;
                    } else {
                        ctx.lineTo(screenX, screenY);
                    }
                }
            }
            
            ctx.stroke();
        }
        
        function drawSigmoid(canvas) {
            drawFunction(canvas, x => 1 / (1 + Math.exp(-x)));
        }
        
        function drawTanh(canvas) {
            drawFunction(canvas, x => Math.tanh(x));
        }
        
        function drawReLU(canvas) {
            drawFunction(canvas, x => Math.max(0, x));
        }
        
        function drawLeakyReLU(canvas) {
            drawFunction(canvas, x => x > 0 ? x : 0.01 * x);
        }
        
        function drawELU(canvas) {
            const alpha = 1;
            drawFunction(canvas, x => x > 0 ? x : alpha * (Math.exp(x) - 1));
        }
        
        function drawSwish(canvas) {
            drawFunction(canvas, x => x / (1 + Math.exp(-x)));
        }
        
        // Initialize
        showSlide(0);
        
        // Redraw graphs on window resize
        window.addEventListener('resize', () => {
            setTimeout(drawGraphs, 100);
        });
    </script>
</body>
</html> padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                        <h4>❌ Sigmoid בשכבות נסתרות</h4>
                        <p>יגרום ל-Vanishing Gradient ברשתות עמוקות</p>
                    </div>
                    
                    <div style="background: #fdeaea;